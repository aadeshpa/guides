---
permalink: /guides/app-monitoring-ocp4.2/
---
:projectid: ocp-app-monitoring
:page-layout: guide-multipane
:page-duration: 60 minutes
:page-releasedate: 2020-03-26
:page-guide-category: basic
:page-description: Learn how to monitor applications on RHOCP 4.2 or RHOCP 4.3 with Prometheus and Grafana.
:page-tags: ['monitoring', 'prometheus', 'grafana']
= Application Monitoring on Red Hat OpenShift Container Platform (RHOCP) 4.3 with Prometheus and Grafana

== Introduction

__The following guide has been tested with RHOCP 4.2/Kabanero 0.3.0 and RHOCP 4.3/Kabanero 0.6.0.__


For application monitoring on RHOCP, you need to set up your own Prometheus and Grafana deployments. Both Prometheus and Grafana can be set up via Operator Lifecycle Manager (OLM).

=== Deploy an Application with MP Metrics Endpoint
Prior to deploying Prometheus, ensure that there is a running application that has a service endpoint for outputting metrics in Prometheus format. 

It is assumed such a running application has been deployed to the RHOCP cluster inside a project/namespace called `myapp`, and that the Prometheus metrics endpoint is exposed on path `/metrics`.

== Deploy Prometheus - Prometheus Operator

service_monitor.yaml
[source, yaml, linenums, role='code-column']
----
include::code/service_monitor.yaml[]
----

service_account.yaml
[source, yaml, linenums, role='code-column']
----
include::code/service_account.yaml[]
----

prometheus.yaml
[source, yaml, linenums, role='code-column']
----
include::code/prometheus.yaml[]
----


The Prometheus Operator is an open-source project originating from CoreOS and exists as a part of their Kubernetes Operator framework. When the Prometheus Operator is installed on the Kubernetes system, you no longer need to hand-configure the Prometheus configuration. Instead, you create CoreOS ServiceMonitor resources for each of the service endpoints that needs to be monitored: this makes daily maintenance of the Prometheus server a lot easier. An architecture overview of the Prometheus Operator is shown below:

image::/img/guide/prometheusOperator.png[link="/img/guide/prometheusOperator.png" alt="Prometheus Operator"]

Using Operator Lifecycle Manager (OLM), Prometheus operator can be easily installed and configured in RHOCP Web Console. 

=== Install Prometheus Operator Using Operator Lifecycle Manager (OLM)

The following procedure is based on https://medium.com/faun/using-the-operator-lifecycle-manager-to-deploy-prometheus-on-openshift-cd2f3abb3511[Using the Operator Lifecycle Manager to deploy Prometheus on OpenShift], with the added inclusion of OpenShift commands needed to complete each step.

. Create a new namespace for our Prometheus Operator deployment
+
[role="command"]
----
oc new-project prometheus-operator
----
+
. Go to OpenShift Container Platform web console and Click on Operators > OperatorHub. Using the OLM, Operators can be easily pulled, installed and subscribed on the cluster. Ensure that the Project is set to prometheus-operator. Search for Prometheus Operator and install it. Choose prometheus-operator under __A specific namespace on the cluster__ and subscribe.

. Click on Overview and create a service monitor instance. A ServiceMonitor defines a service endpoint that needs to be monitored by the Prometheus instance.

. Inside the Service Monitor YAML file, make sure *metadata.namespace* is your monitoring namespace. In this case, it will be prometheus-operator. *spec.namespaceSelector* and *spec.selector* for labels should be configured to match your app deployment's namespace and label. For example, inside [hotspot file=0]`service_monitor.yaml` file, an application with label *app: example-app* from namespace **myapp** will be monitored by the service monitor. If the metrics endpoint is secured, you can define a secured endpoint with authentication configuration by following the https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint[endpoint] API documentation of Prometheus Operator.
+
[role="code_command hotspot file=0", subs="quotes"]
----
Refer to the `service_monitor.yaml` file
----
+

. Create a Prometheus Service Account and Cluster Role, and define a Cluster Role Binding to bind the two together. This is to ensure Prometheus has cluster-wide access to the metrics endpoint on pods in other namespaces. Refer to [hotspot file=1]`service_account.yaml`. Create the YAML file and apply it.
+
[role=command]
----
oc apply -f service_account.yaml
----
+

. Click on Overview and create a Prometheus instance. A Prometheus resource can scrape the targets defined in the ServiceMonitor resource.

. Inside the Prometheus YAML file, make sure *metadata.namespace* is prometheus-operator. Ensure *spec.serviceAccountName* is the Service Account's name that you have applied in the previous step. You can set the match expression to select which service monitors you are interested in under *spec.serviceMonitorSelector.matchExpressions* as in [hotspot file=2]`prometheus.yaml` file.
+
[role="code_command hotspot file=2", subs="quotes"]
----
Refer to the `prometheus.yaml` file
----
+
. Verify that the Prometheus services have successfully started. 
+
[source,role="no_copy"]
----
[root@rhel7-ocp]# oc get svc -n prometheus-operator
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
prometheus-operated   ClusterIP   None             <none>        9090/TCP         19h
----
+
. Check the server logs from one of the target pods to see if the services are running properly.
+
[source,role="no_copy"]
----
[root@rhel7-ocp]# oc get pods -n prometheus-operator
NAME                                   READY     STATUS    RESTARTS   AGE
prometheus-operator-7fccbd7c74-48m6v   1/1       Running   0          19h
prometheus-prometheus-0                3/3       Running   1          19h
prometheus-prometheus-1                3/3       Running   1          19h
[root@rhel7-ocp]# oc logs prometheus-prometheus-0 -c prometheus -n prometheus-operator
----
+

. Expose the prometheus-operated service to use the Prometheus console externally.
+
[source,role="no_copy"]
----
[root@rhel7-ocp]# oc expose svc/prometheus-operated -n prometheus-operator
route.route.openshift.io/prometheus-operated exposed
[root@rhel7-ocp]# oc get route -n prometheus-operator
NAME         HOST/PORT                                                 PATH      SERVICES     PORT      TERMINATION   WILDCARD
prometheus   prometheus-prometheus-operator.apps.9.37.135.153.nip.io             prometheus   web                     None
----
+
. Visit the Prometheus route and go to the Prometheus targets page. 
Check to see that the Prometheus targets page is picking up the target endpoints.

image::/img/guide/prometheus_endpoints.png[link="/img/guide/prometheus_endpoints.png" alt="Prometheus Target Page"]


== Deploy Grafana

grafana_datasource.yaml
[source, yaml, linenums, role='code-column']
----
include::code/grafana_datasource.yaml[]
----

grafana.yaml
[source, yaml, linenums, role='code-column']
----
include::code/grafana.yaml[]
----

grafana_dashboard.yaml
[source, yaml, linenums, role='code-column']
----
include::code/grafana_dashboard.yaml[]
----

Use Grafana dashboards to visualize the metrics. Perform the following steps to deploy Grafana and ensure that Prometheus endpoints are reachable as a data source in Grafana.

. Choose the *same namespace* as Prometheus Operator deployment.
+
[role="command"]
----
oc project prometheus-operator
----
+
. Go to OpenShift Container Platform web console and click Operators > OperatorHub. Search for Grafana Operator and install it. For __A specific namespace on the cluster__, choose prometheus-operator, and subscribe.

. Click Overview and create a Grafana Data Source instance.

. In the Grafana Data Source YAML file, make sure *metadata.namespace* is prometheus-operator. Set *spec.datasources.url* to the URL of the target datasource. For example, inside [hotspot file=0]`grafana_datasource.yaml` file, the Prometheus service is *prometheus-operated* on port *9090*, so the URL is set to __'http://prometheus-operated:9090'__.
+
[role="code_command hotspot file=0", subs="quotes"]
----
Refer to the `grafana_datasource.yaml` file
----
+

. Click Overview and create a Grafana instance.

. In the Grafana YAML file, make sure *metadata.namespace* is prometheus-operator. You can define the match expression to select which Dashboards you are interested in under *spec.dashboardLabelSelector.matchExpressions*. For example, inside [hotspot file=1]`grafana.yaml` file, the Grafana will discover dashboards with app labels having a value of *grafana*.
+
[role="code_command hotspot file=1", subs="quotes"]
----
Refer to the `grafana.yaml` file
----
+

. Click Overview and create a Grafana Dashboard instance.

. Copy [hotspot file=2]`grafana_dashboard.yaml` to Grafana Dashboard YAML file to check the Data Source is connected and Prometheus endpoints are discoverable.

+
[role="code_command hotspot file=2", subs="quotes"]
----
Apply `grafana_dashboard.yaml` file to check
----
+

. Click on Networking > Routes and go to Grafana's location to see the sample dashboard. You can now consume all the application metrics gathered by Prometheus on the Grafana dashboard.

image::/img/guide/template_grafana_dashboard.png[link="/img/guide/template_grafana_dashboard.png" alt="Template Dashboard"]

[start=10]

. When importing your own Grafana dashboard, your dashboard should be configured under *spec.json* in Grafana Dashboard YAML file. Make sure under *"__inputs"*, the name matches with your Grafana Data Source's *spec.datasources*. For example, inside [hotspot file=2]`grafana_dashboard.yaml` file, *name* is set to "Prometheus".


== Configure Prometheus Operator to Detect Service Monitors in Other Namespaces

By default, the Prometheus Operator only watches the namespace it currently resides in, so in order to get the Prometheus Operator to detect service monitors created in other namespaces, you must apply the following configuration changes.

. In your monitoring namespace - in this case, the monitoring namespace is `prometheus-operator` - edit the OperatorGroup to add your application's namespace, for example, `myapp`, to the list of targeted namesaces to be watched. This will change the *olm.targetNamespaces* variable that the Prometheus Operator uses for detecting namespaces to include your `myapp` namespace.
+
[role="command"]
----
oc edit operatorgroup
----
+

+
[source,role="no_copy"]
----
spec:
  targetNamespaces:
  - prometheus-operator
  - myapp
----
+

. Since we have changed the `prometheus-operator` namespace's OperatorGroup to monitor more than one namespace, the operators in this namespace must have the *MultiNamespace* installMode set to *true*. Prometheus Operator installed via OLM has the *MultiNamespace* installMode set to *false* by default, disabling monitoring for more than one namespace, so this must be changed to *true*.
+
[role="command"]
----
oc edit csv prometheusoperator.0.32.0
----
+

+
[source,role="no_copy"]
----
spec:
  installModes:
  - supported: true
    type: OwnNamespace
  - supported: true
    type: SingleNamespace
  - supported: true       # this line should be true
    type: MultiNamespace
  - supported: false
    type: AllNamespaces
----
+

. The same goes for the Grafana Operator, the *MultiNamespace* installMode should be set to *true*; edit the operator using:
+
[role="command"]
----
oc edit csv grafana-operator.v2.0.0 
----
+

. Edit the Prometheus instance to add the *serviceMonitorNamespaceSelector* definition. The empty brackets *{}* allow Prometheus to scrape from *all* namespaces:
+
[role="command"]
----
oc edit prometheuses.monitoring.coreos.com prometheus
----
+

+
[source,role="no_copy"]
----
spec:
  serviceMonitorNamespaceSelector: {}
----
+

. Restart the Prometheus Operator and Grafana Operator pods to see the changes.

== Installation Complete

You now have the Prometheus and Grafana stack installed and configured to monitor your applications. Import custom dashboards and visit the Grafana route to see your metrics visualized.